<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Ecto-Trigger: Usage Guide: How to Use Ecto-Trigger</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Ecto-Trigger<span id="projectnumber">&#160;0.2</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('md_guides_2usage.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Usage Guide: How to Use Ecto-Trigger</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md11"></a></p>
<h1><a class="anchor" id="autotoc_md12"></a>
Overview</h1>
<p>Ecto-Trigger comprises a basic [collection of files](&lt;redacted&gt;). Where each file defines a class and command-line interface for independent tasks. In other words, it is made up of a small number of Python scripts, that each do one job. Ecto-Trigger contains files for training a model, evaluating its performance, preparing to run it on a field device or computing an example saliency map for a given model and image.</p>
<p>Each section in the guide below explains how to use each of the scripts in the Ecto-Trigger toolkit. All the tools are modular, so you can use them together, or independently depending on your needs. Each script is also well documented with comments to make it easy to extend.</p>
<h1><a class="anchor" id="autotoc_md13"></a>
<a href="../model_loader.py">model_loader.py</a></h1>
<p>This script provides a class, <code>ModelLoader</code>, which has static methods <code>create_model()</code>, <code>load_keras_model()</code> and <code>load_tflite_model()</code> to create, load or prepare models for use in training, evaluation or inference. While you won't typically use this file directly from the command line, its essential for internal use throughout the Ecto-Trigger workflow.</p>
<h2><a class="anchor" id="autotoc_md14"></a>
What it does</h2>
<ul>
<li>Creates new models based on configurable parameters (e.g. image shape, or width multiplier)</li>
<li>Loads trained Keras models from <code>.hdf5</code> files</li>
<li>Loads quantised TensorFlow Lite models (<code>.tflite</code>)</li>
</ul>
<h2><a class="anchor" id="autotoc_md15"></a>
Python usage example</h2>
<p>Below is shown how to use the <code>ModelLoader</code> class via a Python programme.</p>
<div class="fragment"><div class="line"><span class="keyword">from</span> model_loader <span class="keyword">import</span> ModelLoader</div>
<div class="line"> </div>
<div class="line"><span class="comment"># this line will load a Keras model from a .hdf5 checkpoint file</span></div>
<div class="line">k_model = ModelLoader.load_keras_model(<span class="stringliteral">&quot;path/to/weights.hdf5&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># this line will load a compressed tflite model from a .tflite file</span></div>
<div class="line">q_model = ModelLoader.load_tflite_model(<span class="stringliteral">&quot;path/to/weights.tflite&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># to create your own model, use the function below</span></div>
<div class="line">k_model = ModelLoader.create_model((1080, 1080, 3), 0.5, dropout_rate=0.2, freeze_base=<span class="keyword">False</span>)</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md16"></a>
Inputs and Outputs</h2>
<div class="fragment"><div class="line">&gt; Input:</div>
<div class="line">  - A model file (.hdf5 or .tflite), or model creation parameters</div>
<div class="line"> </div>
<div class="line">&gt; Output:</div>
<div class="line">  - A TensorFlow Keras model object (for training/evaluation), or</div>
<div class="line">  - A TensorFlow Lite interpreter object (for on-device inference)</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md17"></a>
<a href="../model_trainer.py">model_trainer.py</a></h1>
<p>This script contains a class, <code>ModelTrainer</code>, which allows you to train a binary classification model using your own dataset of labelled images. You can train directly in Python or from the command line.</p>
<p>It supports custom parameterisation, so you can change the input size or model width (see [our paper]() to understand how these affected our trained models). It saves the <code>hdf5</code> model along with logs throughout the training process, which can be monitored using TensorBoard.</p>
<h2><a class="anchor" id="autotoc_md18"></a>
What it does</h2>
<ul>
<li>Loads images and YOLO-format labels from your training and validation directories</li>
<li>Builds and compiles a MobileNetv2-based model for binary classifiation of your data</li>
<li>Trains the model for a given number of epochs</li>
<li>Logs performance metrics (such as accuracy) to a directory for visualisation with TensorBoard</li>
<li>Saves the trained model as a <code>.hdf5</code> file</li>
</ul>
<p>You can load the <code>ModelTrainer</code> class from a Python programme as follows:</p>
<div class="fragment"><div class="line"><span class="keyword">from</span> model_trainer <span class="keyword">import</span> ModelTrainer</div>
<div class="line"> </div>
<div class="line">mt = ModelTrainer({<span class="stringliteral">&quot;train_data_dir&quot;</span>: <span class="stringliteral">&quot;/path/to/train/data&quot;</span>, <span class="stringliteral">&quot;val_data_dir&quot;</span>: <span class="stringliteral">&quot;/path/to/val/data&quot;</span>, <span class="stringliteral">&quot;batch_size&quot;</span> : 16, <span class="stringliteral">&quot;input_shape&quot;</span> : (120,160,3), <span class="stringliteral">&quot;alpha&quot;</span>: 0.35, <span class="stringliteral">&quot;log_dir&quot;</span> : <span class="stringliteral">&quot;logs&quot;</span>, <span class="stringliteral">&quot;model_type&quot;</span>: <span class="stringliteral">&quot;Mobnetv2&quot;</span>, <span class="stringliteral">&quot;epochs&quot;</span> : 2, <span class="stringliteral">&quot;use_pretrained_weights&quot;</span> : <span class="keyword">False</span>})</div>
<div class="line"> </div>
<div class="line">mt.train()</div>
</div><!-- fragment --><p>Alternatively, you can also call <code>model_trainer.py</code> directly from the command line:</p>
<div class="fragment"><div class="line">python model_trainer.py \</div>
<div class="line">    --train_data_dir &quot;/path/to/train/data&quot; \</div>
<div class="line">    --val_data_dir &quot;/path/to/validation/data&quot; \</div>
<div class="line">    --batch_size 16 \</div>
<div class="line">    --input_shape &quot;(120, 160, 3)&quot; \</div>
<div class="line">    --alpha 0.35 \</div>
<div class="line">    --log_dir &quot;logs&quot; \</div>
<div class="line">    --epochs 20 \</div>
</div><!-- fragment --><p>Where:</p>
<ul>
<li><code>train_data_dir</code> is the path to your directory of training images and YOLO-format labels</li>
<li><code>val_data_dir</code> is the path to your directory of validation images and YOLO-format labels</li>
<li><code>batch_size</code> is the number of images per training step (e.g. 16)</li>
<li><code>input_shape</code> is the dimensions for the input size, specified as a string tuple, e.g. (120, 160, 3), height, width, number of channels. <br  />
</li>
<li><code>alpha</code> controls the model size through changing the model width - smaller values of alpha produces lighter models, but this has an accuracy penalty, see our paper for further details.</li>
<li><code>log_dir</code> is the directory where training logs will be saved for analysis with TensorBoard.</li>
<li><code>epochs</code> is the number of full passes though the training data to run before exiting training.</li>
</ul>
<h2><a class="anchor" id="autotoc_md19"></a>
Inputs and Outputs</h2>
<div class="fragment"><div class="line">&gt; Input:</div>
<div class="line">  - Folder of labelled training images (YOLO format)</div>
<div class="line">  - Folder of labelled validation images (YOLO format)</div>
<div class="line"> </div>
<div class="line">&gt; Output:</div>
<div class="line">  - Trained `.hdf5` model file</div>
<div class="line">  - Training logs viewable in TensorBoard</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md20"></a>
YOLO Dataset Format</h2>
<p>To organise your dataset in YOLO-format for training, follow this structure:</p>
<div class="fragment"><div class="line">your_data_train/</div>
<div class="line">|-- img001.jpg</div>
<div class="line">|--  img001.txt</div>
<div class="line">|--  img002.jpg</div>
<div class="line">|--  img002.txt</div>
<div class="line">|--  ...</div>
<div class="line"> </div>
<div class="line">your_data_val/</div>
<div class="line">|--  img001.jpg</div>
<div class="line">|--  img001.txt</div>
<div class="line">|--  img002.jpg</div>
<div class="line">|--  img002.txt</div>
<div class="line">|--  ...</div>
</div><!-- fragment --><p> Each <code>.txt</code> file should contain the YOLO-style annotation for its corresponding image (for more information about the YOLO format, see <a href="https://roboflow.com/formats/yolo-darknet-txt">here</a>):</p>
<p>For images with the object of interest: each line in the .txt file should contain a bounding box in this format:</p>
<div class="fragment"><div class="line">0 x_center y_center width height</div>
</div><!-- fragment --><p>For images without the object: the .txt file will be empty (zero length).</p>
<h2><a class="anchor" id="autotoc_md21"></a>
Monitoring Training with TensorBoard</h2>
<p>You can check training progress using <code>Tensorboard</code> by passing it the log directory:</p>
<div class="fragment"><div class="line">tensorboard --logdir=logs</div>
</div><!-- fragment --><p>This allows you to monitor accuracy values throughout training, so you can see how its going and when you might want to stop model training.</p>
<h1><a class="anchor" id="autotoc_md22"></a>
<a href="../model_evaluator.py">model_evaluator.py</a></h1>
<p>This script contain a class, <code>ModelEvaluator</code>, which helps you evaluate how well your trained model performas on a labelled test dataset. It calculates and prints out the model's accuracy and loss, giving you a quick sense of how confidently and correctly your model is making predictions.</p>
<h2><a class="anchor" id="autotoc_md23"></a>
What it does</h2>
<ul>
<li>Loads a trained Keras model (given in <code>.hdf5</code>) format.</li>
<li>Loads a test dataset from a directory in YOLO-format.</li>
<li>Evaluates the models performance on the dataset.</li>
<li>Prints out the accuracy and loss values to the terminal window.</li>
</ul>
<h2><a class="anchor" id="autotoc_md24"></a>
Python usage example</h2>
<p>You can call the <code>ModelEvaluator</code> directly inside your Python programmes:</p>
<div class="fragment"><div class="line"><span class="keyword">from</span> model_evaluator <span class="keyword">import</span> ModelEvaluator</div>
<div class="line"> </div>
<div class="line">me = ModelEvaluator(16, (120, 160, 3), <span class="stringliteral">&quot;model_weights/8/checkpoints/weights.10.hdf5&quot;</span>, <span class="stringliteral">&quot;/path/to/validation/data&quot;</span>) </div>
<div class="line"> </div>
<div class="line">me.evaluate()</div>
</div><!-- fragment --><p>You can also call <code>model_evaluator.py</code> from the command line:</p>
<div class="fragment"><div class="line">python model_evaluator.py \</div>
<div class="line">    --batch_size 16 \</div>
<div class="line">    --weights_path &quot;model_weights/8/checkpoints/weights.10.hdf5&quot; \</div>
<div class="line">    --test_data_dir &quot;/path/to/validation/data&quot;</div>
</div><!-- fragment --><p>Where:</p>
<ul>
<li><code>batch_size</code> is the number of images to process per step (e.g. 16)</li>
<li><code>weights_path</code> is the path to the trained Keras model to evaluate (<code>.hdf5</code> format)</li>
<li><code>test_data_dir</code> is the folder of labelled images in YOLO format for evaluation</li>
</ul>
<h2><a class="anchor" id="autotoc_md25"></a>
Inputs and Outputs</h2>
<div class="fragment"><div class="line">&gt; Input: Trained `.hdf5` model, test image folder </div>
<div class="line">&gt; Output: Accuracy score, loss printed to terminal window</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md26"></a>
<a href="../model_quantiser.py">model_quantiser.py</a></h1>
<p>This script defines a class, <code>ModelQuantiser</code> which can be instantiated in Python as shown to quantise a given model in keras <code>.hdf5</code> format and save it as a <code>.tflite</code> file. Quantisation helps you convert an Ecto-Trigger Keras model into a smaller, more computationally efficient representation for use on low-powered devices such as Raspberry Pi or ESP32-S3. This allows real-time, on-device object detection in the field.</p>
<h2><a class="anchor" id="autotoc_md27"></a>
What it does</h2>
<ul>
<li>Loads a trained Keras model (supplied in <code>.hdf5</code> format)</li>
<li>Calibrates the quantisation process using a representative dataset (a folder containing a small number (around 100) of example images)</li>
<li>Converts the <code>.hdf5</code> model into TensorFlow Lite (<code>.tflite</code>) format.</li>
<li>Saves the now quantised model, ready for deployment. <br  />
</li>
</ul>
<h2><a class="anchor" id="autotoc_md28"></a>
Python usage example</h2>
<p>You can call the <code>ModelQuantiser</code> directly inside your Python programmes:</p>
<div class="fragment"><div class="line"><span class="keyword">from</span> model_quantiser <span class="keyword">import</span> ModelQuantiser</div>
<div class="line"> </div>
<div class="line">mq = ModelQuantiser(</div>
<div class="line">        weights_file=<span class="stringliteral">&quot;/path/to/weights.hdf5&quot;</span>,</div>
<div class="line">        representative_dataset=<span class="stringliteral">&quot;/path/to/representative_dataset,</span></div>
<div class="line"><span class="stringliteral">        representative_example_nr=100</span></div>
<div class="line"><span class="stringliteral">    )</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral"></span><span class="comment"># Quantise the model and save it</span></div>
<div class="line">mq.quantise_model(output_path=<span class="stringliteral">&quot;/path/to/weights.tflite&quot;</span>)</div>
</div><!-- fragment --><p>You can also call <code>model_quantiser.py</code> from the command line:</p>
<div class="fragment"><div class="line">python model_quantiser.py \</div>
<div class="line">  --weights_file /path/to/weights.hdf5 \</div>
<div class="line">  --representative_dataset /path/to/representative_dataset \</div>
<div class="line">  --representative_example_nr 100 \</div>
<div class="line">  --output /path/to/weights.tflite</div>
</div><!-- fragment --><p>Where:</p>
<ul>
<li><code>weights_file</code> is the path to the trained Keras model (<code>.hdf5</code>) file, to be quantised.</li>
<li><code>representative_dataset</code> is the path to the folder of sample images to use for calibrating the quantisation process</li>
<li><code>representative_example_nr</code> is the number of images to use from that folder, e.g. 100.</li>
<li><code>output</code> is the filepath for saving the resultant <code>.tflite</code> model.</li>
</ul>
<h2><a class="anchor" id="autotoc_md29"></a>
Inputs and Outputs</h2>
<div class="fragment"><div class="line">&gt; Input:</div>
<div class="line">  - A trained `.hdf5` model</div>
<div class="line">  - A validation dataset (images and matching YOLO-style `.txt` files)</div>
<div class="line"> </div>
<div class="line">&gt; Output:</div>
<div class="line">  - Accuracy and loss values printed to the terminal</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md30"></a>
<a href="../saliency_map_evaluator.py">saliency_map_evaluator.py</a></h1>
<p>This script provides a visual way to understand what the model is "looking at" when it makes a prediction. It generates saliency maps, these are heatmaps that highlight the parts of a given input image which most influenced a model's decision. This is a helpful tool for interpretation, debugging and visualisations.</p>
<h2><a class="anchor" id="autotoc_md31"></a>
What it does</h2>
<ul>
<li>Loads a given Keras model (given to the script in <code>.hdf5</code> format)</li>
<li>Processes a given input image (given to the script in <code>.jpg</code> or <code>.png</code> format)</li>
<li>Runs the model and computes a saliency map showing where the model focused.</li>
<li>Creates a composite image which includes: the original image, the saliency heatmap, the confidence score at the output of the model.</li>
</ul>
<h2><a class="anchor" id="autotoc_md32"></a>
Python usage example</h2>
<p>If you want to use the script in your own programmes, you can call it programmatically as follows:</p>
<div class="fragment"><div class="line"><span class="keyword">from</span> saliency_map_evaluator <span class="keyword">import</span> SaliencyMapGenerator</div>
<div class="line"> </div>
<div class="line">smg = SaliencyMapGenerator(weights_file=<span class="stringliteral">&quot;model_weights/8/checkpoints/weights.10.hdf5&quot;</span>)</div>
<div class="line"> </div>
<div class="line">smg.generate_saliency_map(input_image_path=<span class="stringliteral">&quot;input.png&quot;</span>, output_path=<span class="stringliteral">&quot;saliency_plot.png&quot;</span>)</div>
</div><!-- fragment --><p>You can also call <code>saliency_map_evaluator.py</code> from the command line:</p>
<div class="fragment"><div class="line">python saliency_map_evaluator.py \</div>
<div class="line">  --weights_file model_weights/8/checkpoints/weights.10.hdf5 </div>
<div class="line">  --input_image input.png \</div>
<div class="line">  --output saliency_plot.png</div>
</div><!-- fragment --><p>Where:</p><ul>
<li><code>weights_file</code> is the path to your trained model file in <code>.hdf5</code> format</li>
<li><code>input_image</code> is the path to the image you wish to analyse</li>
<li><code>output</code> defines the filename and path you want to save the final composite plot to.</li>
</ul>
<h2><a class="anchor" id="autotoc_md33"></a>
Inputs and Outputs</h2>
<div class="fragment"><div class="line">&gt; Input:</div>
<div class="line">  - A trained model file (.hdf5)</div>
<div class="line">  - An image file (e.g., .jpg or .png)</div>
<div class="line"> </div>
<div class="line">&gt; Output:</div>
<div class="line">  - A .png image showing:</div>
<div class="line">      - The input image</div>
<div class="line">      - A heatmap of attention (saliency map)</div>
<div class="line">      - The prediction confidence score</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md34"></a>
<a href="../generator.py">generator.py</a></h1>
<p>This file defines the class <code>CustomDataGenerator</code>, which is a data loading utility built for training Ecto-Trigger models. It works with image datasets which follow the YOLO annoation format (as described above), and prepared them for binary classification tasks (e.g. "insect" or "no insect").</p>
<h2><a class="anchor" id="autotoc_md35"></a>
What it does</h2>
<p><code>CustomDataGenerator</code> creates batches of images and labels which can be used by other scripts for both training and evaluation. It:</p>
<ul>
<li>Loads images from a folder, which is specified as an argument.</li>
<li>Matches each image with its corresponding <code>.txt</code> annotation file in YOLO-style.</li>
<li>Converts all annotations for a given image into binary labels (1 = object present, 0 = no object).</li>
<li>Resizes and formats the images for input into the model.</li>
<li>Optionally shuffles and augments the images (see our paper for details on augmentations).</li>
<li>Supplies batches of images and labels to a model, via a Keras-compatible interface.</li>
</ul>
<p>This is useful when training models using <code>model_trainer.py</code>.</p>
<h2><a class="anchor" id="autotoc_md36"></a>
Python usage example</h2>
<p>If you want to integrate <code>CustomDataGenerator</code> into your own programmes, you can use it inside Python as follows: </p><div class="fragment"><div class="line"><span class="keyword">from</span> generator <span class="keyword">import</span> CustomDataGenerator</div>
<div class="line">data_gen = CustomDataGenerator(</div>
<div class="line">    data_directory=<span class="stringliteral">&quot;/path/to/dataset&quot;</span>,    <span class="comment">#Folder with .jpg and .txt files in YOLO-format, this is your dataset</span></div>
<div class="line">    batch_size=16,                        <span class="comment">#Number of images per batch</span></div>
<div class="line">    input_shape=(224, 224, 3),            <span class="comment">#Input shape to resize all images to</span></div>
<div class="line">    stop_training_flag={<span class="stringliteral">&quot;stop&quot;</span>: <span class="keyword">False</span>},   <span class="comment">#Flag for manually stopping training (optional) </span></div>
<div class="line">    shuffle=<span class="keyword">True</span>                          <span class="comment">#Shuffle the dataset after each epoch</span></div>
<div class="line">)</div>
<div class="line"><span class="comment">#To get an example batch image images and labels</span></div>
<div class="line">X, y = data_gen[0] <span class="comment">#Returns X, a batch of images, and y, a batch of 0s and 1s (i.e. the labels)</span></div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md37"></a>
Inputs and Outputs</h2>
<div class="fragment"><div class="line">&gt; Input:</div>
<div class="line">  - Folder of .jpg images</div>
<div class="line">  - Corresponding .txt files (YOLO format):</div>
<div class="line">      - If object is present: &quot;0 x_center y_center width height&quot;</div>
<div class="line">      - If object is absent: (empty .txt file)</div>
<div class="line"> </div>
<div class="line">&gt; Output:</div>
<div class="line">  - X: NumPy array of shape (batch_size, height, width, channels)</div>
<div class="line">  - y: NumPy array of binary labels (1 if object present, 0 if not)</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md38"></a>
Suggested Workflow</h1>
<p>To use each of these files together to create, train, evaluate and deploy a model, you would follow the order of the workflow below:</p><ol type="1">
<li>Prepare your dataset in class-based folders</li>
<li>Train a model (<code>model_trainer.py</code>)</li>
<li>Evaluate it (<code>model_evaluator.py</code>)</li>
<li>Quantise for deployment (<code>model_quantiser.py</code>)</li>
<li>Deploy to field hardware (<a class="el" href="md_guides_2deployment.html">Deployment Guide</a>)</li>
<li>Optionally visualize attention maps (<code>saliency_map_evaluator.py</code>) </li>
</ol>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
